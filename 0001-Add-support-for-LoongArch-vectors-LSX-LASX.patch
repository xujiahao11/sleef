From 1b6d9133d77e9b86d1b1e5b9844fddb9b30a094a Mon Sep 17 00:00:00 2001
From: Jiahao Xu <xujiahao@loongson.cn>
Date: Thu, 7 Dec 2023 19:14:21 +0800
Subject: [PATCH 1/2] Add support for LoongArch vectors: LSX, LASX

The patch enables SLEEF to use LoongArch vector extension, and add the relevant configuration
settings in order to compile sleef On LoongArch.

* Not supported yet
- DFT, QUAD, and CI.

diff --git a/CMakeLists.txt b/CMakeLists.txt
index c38219f..2469cab 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -47,6 +47,7 @@ set(SLEEF_ALL_SUPPORTED_EXTENSIONS
   VSX VSXNOFMA VSX3 VSX3NOFMA                           # PPC64
   VXE VXENOFMA VXE2 VXE2NOFMA	                        # IBM Z
   RVVM1NOFMA RVVM1 RVVM2NOFMA RVVM2                     # RISC-V Vectors
+  LASX LSX                                              # LoongArch
   PUREC_SCALAR PURECFMA_SCALAR                          # Generic type
   CACHE STRING "List of SIMD architectures supported by libsleef."
   )
@@ -58,11 +59,12 @@ set(SLEEF_SUPPORTED_LIBM_EXTENSIONS
   VSX VSXNOFMA VSX3 VSX3NOFMA                           # PPC64
   VXE VXENOFMA VXE2 VXE2NOFMA	                        # IBM Z
   RVVM1NOFMA RVVM1 RVVM2NOFMA RVVM2                     # RISC-V Vectors
+  LASX LSX                                              # LoongArch
   PUREC_SCALAR PURECFMA_SCALAR                          # Generic type
   CACHE STRING "List of SIMD architectures supported by libsleef."
   )
 set(SLEEF_SUPPORTED_GNUABI_EXTENSIONS 
-  SSE2 AVX AVX2 AVX512F ADVSIMD SVE
+  SSE2 AVX AVX2 AVX512F ADVSIMD SVE LASX LSX
   CACHE STRING "List of SIMD architectures supported by libsleef for GNU ABI."
 )
 
@@ -91,6 +93,8 @@ set(COSTOVERRIDE_RVVM1 10)
 set(COSTOVERRIDE_RVVM1NOFMA 10)
 set(COSTOVERRIDE_RVVM2 20)
 set(COSTOVERRIDE_RVVM2NOFMA 20)
+set(COSTOVERRIDE_LASX 2)
+set(COSTOVERRIDE_LSX 2)
 
 #
 
diff --git a/Configure.cmake b/Configure.cmake
index 58fe540..340cefd 100644
--- a/Configure.cmake
+++ b/Configure.cmake
@@ -122,6 +122,11 @@ elseif(CMAKE_SYSTEM_PROCESSOR MATCHES "s390x")
   set(CLANG_FLAGS_ENABLE_PURECFMA_SCALAR "-march=z14;-mzvector")
 elseif(CMAKE_SYSTEM_PROCESSOR MATCHES "riscv64")
   set(SLEEF_ARCH_RISCV64 ON CACHE INTERNAL "True for RISCV64 architecture.")
+
+elseif(CMAKE_SYSTEM_PROCESSOR MATCHES "loongarch64")
+  set(SLEEF_ARCH_LOONGARCH64 ON CACHE INTERNAL "True for loongarch64 architecture.")
+  set(COMPILER_SUPPORTS_LSX 1)
+  set(COMPILER_SUPPORTS_LASX 1)
 endif()
 
 set(COMPILER_SUPPORTS_PUREC_SCALAR 1)
@@ -171,6 +176,9 @@ set(CLANG_FLAGS_ENABLE_RVVM1 "-march=rv64gcv_zba_zbb_zbs")
 set(CLANG_FLAGS_ENABLE_RVVM1NOFMA "-march=rv64gcv_zba_zbb_zbs")
 set(CLANG_FLAGS_ENABLE_RVVM2 "-march=rv64gcv_zba_zbb_zbs")
 set(CLANG_FLAGS_ENABLE_RVVM2NOFMA "-march=rv64gcv_zba_zbb_zbs")
+# LoongArch vector extensions.
+set(CLANG_FLAGS_ENABLE_LASX "-mlasx")
+set(CLANG_FLAGS_ENABLE_LSX "-mlsx")
 
 set(FLAGS_OTHERS "")
 
@@ -668,6 +676,42 @@ if (ENFORCE_RVVM2 AND NOT COMPILER_SUPPORTS_RVVM2)
   message(FATAL_ERROR "ENFORCE_RVVM2 is specified and that feature is disabled or not supported by the compiler")
 endif()
 
+# LASX
+
+option(DISABLE_LASX "Disable LASX" OFF)
+option(ENFORCE_LASX "Build fails if LASX is not supported by the compiler" OFF)
+
+if(SLEEF_ARCH_LOONGARCH64 AND NOT DISABLE_LASX)
+  string (REPLACE ";" " " CMAKE_REQUIRED_FLAGS "${FLAGS_ENABLE_LASX}")
+  CHECK_C_SOURCE_COMPILES("
+  #include <lasxintrin.h>
+  int main() {
+    __m256d r;
+  }" COMPILER_SUPPORTS_LASX)
+endif()
+
+if (ENFORCE_LASX AND NOT COMPILER_SUPPORTS_LASX)
+  message(FATAL_ERROR "ENFORCE_LASX is specified and that feature is disabled or not supported by the compiler")
+endif()
+
+# LSX
+
+option(DISABLE_LSX "Disable LSX" OFF)
+option(ENFORCE_LSX "Build fails if LSX is not supported by the compiler" OFF)
+
+if(SLEEF_ARCH_LOONGARCH64 AND NOT DISABLE_LSX)
+  string (REPLACE ";" " " CMAKE_REQUIRED_FLAGS "${FLAGS_ENABLE_LSX}")
+  CHECK_C_SOURCE_COMPILES("
+  #include <lsxintrin.h>
+  int main() {
+    __m128d r;
+  }" COMPILER_SUPPORTS_LSX)
+endif()
+
+if (ENFORCE_LSX AND NOT COMPILER_SUPPORTS_LSX)
+  message(FATAL_ERROR "ENFORCE_LSX is specified and that feature is disabled or not supported by the compiler")
+endif()
+
 # CUDA
 
 option(ENFORCE_CUDA "Build fails if CUDA is not supported" OFF)
diff --git a/src/arch/helperlasx.h b/src/arch/helperlasx.h
new file mode 100644
index 0000000..1d57c5a
--- /dev/null
+++ b/src/arch/helperlasx.h
@@ -0,0 +1,350 @@
+#define ENABLE_DP
+//@#define ENABLE_DP
+#define LOG2VECTLENDP 2
+//@#define LOG2VECTLENDP 2
+#define VECTLENDP (1 << LOG2VECTLENDP)
+//@#define VECTLENDP (1 << LOG2VECTLENDP)
+#define ENABLE_FMA_DP
+//@#define ENABLE_FMA_DP
+
+#define ENABLE_SP
+//@#define ENABLE_SP
+#define LOG2VECTLENSP (LOG2VECTLENDP+1)
+//@#define LOG2VECTLENSP (LOG2VECTLENDP+1)
+#define VECTLENSP (1 << LOG2VECTLENSP)
+//@#define VECTLENSP (1 << LOG2VECTLENSP)
+#define ENABLE_FMA_SP
+//@#define ENABLE_FMA_SP
+
+#define FULL_FP_ROUNDING
+//@#define FULL_FP_ROUNDING
+#define ACCURATE_SQRT
+//@#define ACCURATE_SQRT
+
+#define ISANAME "LASX"
+#define DFTPRIORITY 25
+
+#include <lasxintrin.h>
+
+#include <stdint.h>
+#include "misc.h"
+
+
+typedef __m256i vmask;
+typedef __m256i vopmask;
+
+typedef __m256d vdouble;
+typedef __m256i vint;
+
+typedef __m256 vfloat;
+typedef __m256i vint2;
+
+typedef __m256i vint64;
+typedef __m256i vuint64;
+
+typedef struct {
+  vmask x, y;
+} vquad;
+
+typedef vquad vargquad;
+
+static INLINE int vavailability_i(int name) { return 3; }
+
+static INLINE void vprefetch_v_p(const void *ptr) { }
+
+static INLINE int vtestallones_i_vo32(vopmask g) {  return __lasx_xbnz_w(g); }
+
+static INLINE int vtestallones_i_vo64(vopmask g) {  return __lasx_xbnz_d(g); }
+
+
+static INLINE vdouble vcast_vd_d(double d) { return (vdouble){d, d, d, d}; }
+static INLINE vmask vreinterpret_vm_vd(vdouble vd) { return (vmask)vd; }
+static INLINE vdouble vreinterpret_vd_vm(vmask vm) { return (vdouble)vm; }
+
+static INLINE vint2 vloadu_vi2_p(int32_t *p) { return __lasx_xvld(p, 0);}
+static INLINE void vstoreu_v_p_vi2(int32_t *p, vint2 v) { *((vint2*)p) = v;}
+static INLINE vint vloadu_vi_p(int32_t *p) {return __lasx_xvld(p, 0);}
+static INLINE void vstoreu_v_p_vi(int32_t *p, vint v) { *((vint*)p) = v;}
+static INLINE vfloat vload_vf_p(const float *ptr) {return (vfloat)__lasx_xvld((void const *)ptr, 0); }
+static INLINE vfloat vloadu_vf_p(const float *ptr) {return (vfloat)__lasx_xvld((void const *)ptr, 0);}
+static INLINE void vstore_v_p_vf(float *ptr, vfloat v) { *((vfloat*)ptr) = v;}
+static INLINE void vstoreu_v_p_vf(float *ptr, vfloat v) { *((vfloat*)ptr) = v;}
+static INLINE vdouble vload_vd_p(const double *ptr) { return (vdouble) __lasx_xvld((void const *)ptr, 0);}
+static INLINE vdouble vloadu_vd_p(const double *ptr) { return (vdouble) __lasx_xvld((void const *)ptr, 0);}
+static INLINE void vstore_v_p_vd(double *ptr, vdouble v) { *((vdouble*)ptr) = v;}
+static INLINE void vstoreu_v_p_vd(double *ptr, vdouble v) { *((vdouble*)ptr) = v;}
+
+static INLINE vmask vand_vm_vm_vm(vmask x, vmask y) { return __lasx_xvand_v(x, y); }
+static INLINE vmask vandnot_vm_vm_vm(vmask x, vmask y) { return __lasx_xvandn_v(x, y); }
+static INLINE vmask vor_vm_vm_vm(vmask x, vmask y) { return __lasx_xvor_v(x, y); }
+static INLINE vmask vxor_vm_vm_vm(vmask x, vmask y) { return __lasx_xvxor_v(x, y); }
+
+static INLINE vopmask vand_vo_vo_vo(vopmask x, vopmask y) { return __lasx_xvand_v(x, y); }
+static INLINE vopmask vandnot_vo_vo_vo(vopmask x, vopmask y) { return __lasx_xvandn_v(x, y); }
+static INLINE vopmask vor_vo_vo_vo(vopmask x, vopmask y) { return __lasx_xvor_v(x, y); }
+static INLINE vopmask vxor_vo_vo_vo(vopmask x, vopmask y) { return __lasx_xvxor_v(x, y); }
+
+static INLINE vmask vand_vm_vo64_vm(vopmask x, vmask y) { return __lasx_xvand_v(x, y); }
+static INLINE vmask vandnot_vm_vo64_vm(vopmask x, vmask y) { return __lasx_xvandn_v(x, y); }
+static INLINE vmask vor_vm_vo64_vm(vopmask x, vmask y) { return __lasx_xvor_v(x, y); }
+static INLINE vmask vxor_vm_vo64_vm(vopmask x, vmask y) { return __lasx_xvxor_v(x, y); }
+
+static INLINE vmask vand_vm_vo32_vm(vopmask x, vmask y) { return __lasx_xvand_v(x, y); }
+static INLINE vmask vandnot_vm_vo32_vm(vopmask x, vmask y) { return __lasx_xvandn_v(x, y); }
+static INLINE vmask vor_vm_vo32_vm(vopmask x, vmask y) { return __lasx_xvor_v(x, y); }
+static INLINE vmask vxor_vm_vo32_vm(vopmask x, vmask y) { return __lasx_xvxor_v(x, y); }
+
+static INLINE vopmask vcast_vo32_vo64(vopmask o) { return __lasx_xvpermi_d(__lasx_xvpickev_w(o, o), 0xd8);}
+
+static INLINE vopmask vcast_vo64_vo32(vopmask o) { return __lasx_vext2xv_d_w(o);}
+
+static INLINE vopmask vcast_vo_i(int i) {
+  return __lasx_xvreplgr2vr_d(i ? -1 : 0);
+}
+
+
+//towards the nearest even
+static INLINE vdouble vrint_vd_vd(vdouble vd) { return (vdouble)__lasx_xvfrintrne_d(vd); }
+static INLINE vint vrint_vi_vd(vdouble vd) {  return __lasx_xvpermi_d(__lasx_xvftintrne_w_d(vd,vd), 0xd8);}
+static INLINE vfloat vrint_vf_vf(vfloat vd) { return (vfloat)__lasx_xvfrintrne_s(vd); }
+
+//towards zero
+static INLINE vint vtruncate_vi_vd(vdouble vd) {  return __lasx_xvpermi_d(__lasx_xvftintrz_w_d(vd,vd), 0xd8);}
+static INLINE vdouble vtruncate_vd_vd(vdouble vd) { return (vdouble)__lasx_xvfrintrz_d(vd); }
+static INLINE vfloat vtruncate_vf_vf(vfloat vf) { return (vfloat)__lasx_xvfrintrz_s(vf); }
+
+static INLINE vdouble vcast_vd_vi(vint vi) { return  __lasx_xvffint_d_l(__lasx_vext2xv_d_w(vi));}
+static INLINE vint vcast_vi_i(int i) { return __lasx_xvreplgr2vr_w(i); }
+
+static INLINE vmask vcastu_vm_vi(vint vi) { return __lasx_xvslli_d(__lasx_vext2xv_du_wu(vi),32);}
+static INLINE vint vcastu_vi_vm(vmask vi) { return __lasx_xvpermi_d(__lasx_xvpickod_w(vi, vi), 0xd8);}
+static INLINE vmask vcast_vm_i_i(int i0, int i1) {
+  return __lasx_xvilvl_w(__lasx_xvreplgr2vr_w(i0), __lasx_xvreplgr2vr_w(i1));
+}
+
+static INLINE vmask vcast_vm_i64(int64_t i) { return __lasx_xvreplgr2vr_d(i); }
+static INLINE vmask vcast_vm_u64(uint64_t i) { return __lasx_xvreplgr2vr_d((uint64_t)i); }
+
+static INLINE vopmask veq64_vo_vm_vm(vmask x, vmask y) { return __lasx_xvseq_d(x, y); }
+static INLINE vmask vadd64_vm_vm_vm(vmask x, vmask y) { return __lasx_xvadd_d(x, y); }
+
+
+static INLINE vdouble vadd_vd_vd_vd(vdouble x, vdouble y) { return __lasx_xvfadd_d(x, y); }
+static INLINE vdouble vsub_vd_vd_vd(vdouble x, vdouble y) { return __lasx_xvfsub_d(x, y); }
+static INLINE vdouble vmul_vd_vd_vd(vdouble x, vdouble y) { return __lasx_xvfmul_d(x, y); }
+static INLINE vdouble vdiv_vd_vd_vd(vdouble x, vdouble y) { return __lasx_xvfdiv_d(x, y); }
+static INLINE vdouble vrec_vd_vd(vdouble x) { return  __lasx_xvfrecip_d(x); }
+static INLINE vdouble vsqrt_vd_vd(vdouble x) { return __lasx_xvfsqrt_d(x); }
+static INLINE vdouble vabs_vd_vd(vdouble d) { return (vdouble) __lasx_xvandn_v((vmask)vcast_vd_d(-0.0), (vmask)d);}
+static INLINE vdouble vneg_vd_vd(vdouble d) { return (vdouble)__lasx_xvxor_v((vmask)vcast_vd_d(-0.0), (vmask)d); }
+static INLINE vdouble vmla_vd_vd_vd_vd(vdouble x, vdouble y, vdouble z) { return __lasx_xvfmadd_d(x, y, z); }
+static INLINE vdouble vmlapn_vd_vd_vd_vd(vdouble x, vdouble y, vdouble z) { return __lasx_xvfmsub_d(x, y, z); }
+static INLINE vdouble vmlanp_vd_vd_vd_vd(vdouble x, vdouble y, vdouble z) { return __lasx_xvfnmsub_d(x, y, z); }
+static INLINE vdouble vmax_vd_vd_vd(vdouble x, vdouble y) { return __lasx_xvfmax_d(x, y); }
+static INLINE vdouble vmin_vd_vd_vd(vdouble x, vdouble y) { return __lasx_xvfmin_d(x, y); }
+static INLINE vdouble vlogb_vd_vd(vdouble d)  { return __lasx_xvflogb_d(d);}
+
+static INLINE vdouble vfma_vd_vd_vd_vd(vdouble x, vdouble y, vdouble z) { return __lasx_xvfmadd_d(x, y, z); }
+static INLINE vdouble vfmapp_vd_vd_vd_vd(vdouble x, vdouble y, vdouble z) { return __lasx_xvfmadd_d(x, y, z); }
+static INLINE vdouble vfmapn_vd_vd_vd_vd(vdouble x, vdouble y, vdouble z) { return __lasx_xvfmsub_d(x, y, z); }
+static INLINE vdouble vfmanp_vd_vd_vd_vd(vdouble x, vdouble y, vdouble z) { return __lasx_xvfnmsub_d(x, y, z); }
+static INLINE vdouble vfmann_vd_vd_vd_vd(vdouble x, vdouble y, vdouble z) { return __lasx_xvfnmadd_d(x, y, z); }
+
+static INLINE vopmask veq_vo_vd_vd(vdouble x, vdouble y) { return __lasx_xvfcmp_ceq_d(x, y); }
+static INLINE vopmask vneq_vo_vd_vd(vdouble x, vdouble y) { return __lasx_xvfcmp_cne_d(x, y); }
+static INLINE vopmask vlt_vo_vd_vd(vdouble x, vdouble y) { return __lasx_xvfcmp_clt_d(x, y); }
+static INLINE vopmask vle_vo_vd_vd(vdouble x, vdouble y) { return __lasx_xvfcmp_cle_d(x, y); }
+static INLINE vopmask vgt_vo_vd_vd(vdouble x, vdouble y) { return __lasx_xvfcmp_clt_d(y, x);}
+static INLINE vopmask vge_vo_vd_vd(vdouble x, vdouble y) { return __lasx_xvfcmp_clt_d(y, x); }
+
+
+static INLINE vint vadd_vi_vi_vi(vint x, vint y) { return __lasx_xvadd_w(x, y); }
+static INLINE vint vsub_vi_vi_vi(vint x, vint y) { return __lasx_xvsub_w(x, y); }
+static INLINE vint vneg_vi_vi(vint e) { return __lasx_xvneg_w(e); }
+
+static INLINE vint vand_vi_vi_vi(vint x, vint y) { return __lasx_xvand_v(x, y); }
+static INLINE vint vandnot_vi_vi_vi(vint x, vint y) { return __lasx_xvandn_v(x, y); }
+static INLINE vint vor_vi_vi_vi(vint x, vint y) { return __lasx_xvor_v(x, y); }
+static INLINE vint vxor_vi_vi_vi(vint x, vint y) { return __lasx_xvxor_v(x, y); }
+
+static INLINE vint vandnot_vi_vo_vi(vopmask m, vint y) { return __lasx_xvandn_v(m, y); }
+static INLINE vint vand_vi_vo_vi(vopmask m, vint y) { return __lasx_xvand_v(m, y); }
+
+static INLINE vint vsll_vi_vi_i(vint x, int c) { return __lasx_xvslli_w(x, c); }
+static INLINE vint vsrl_vi_vi_i(vint x, int c) { return __lasx_xvsrli_w(x, c); }
+static INLINE vint vsra_vi_vi_i(vint x, int c) { return __lasx_xvsrai_w(x, c); }
+
+static INLINE vint veq_vi_vi_vi(vint x, vint y) { return __lasx_xvseq_w(x, y); }
+static INLINE vint vgt_vi_vi_vi(vint x, vint y) { return __lasx_xvslt_w(y, x);}
+
+static INLINE vopmask veq_vo_vi_vi(vint x, vint y) { return   __lasx_xvseq_w(x, y); }
+static INLINE vopmask vgt_vo_vi_vi(vint x, vint y) { return __lasx_xvslt_w(y, x);}
+
+static INLINE vint vsel_vi_vo_vi_vi(vopmask m, vint x, vint y) { return __lasx_xvbitsel_v(y, x, m);}
+
+static INLINE vdouble vsel_vd_vo_vd_vd(vopmask o, vdouble x, vdouble y) { return (vdouble)__lasx_xvbitsel_v((vmask)y, (vmask)x, o); }
+static INLINE vdouble vsel_vd_vo_d_d(vopmask o, double v1, double v0)
+{
+  return vsel_vd_vo_vd_vd(o, (vdouble){v1, v1, v1, v1}, (vdouble){v0, v0, v0, v0});
+}
+
+static INLINE vdouble vsel_vd_vo_vo_vo_d_d_d_d(vopmask o0, vopmask o1, vopmask o2, double d0, double d1, double d2, double d3) {
+  return vsel_vd_vo_vd_vd(o0, vcast_vd_d(d0), vsel_vd_vo_vd_vd(o1, vcast_vd_d(d1), vsel_vd_vo_d_d(o2,d2, d3)));
+}
+
+static INLINE vdouble vsel_vd_vo_vo_d_d_d(vopmask o0, vopmask o1, double d0, double d1, double d2) {
+  return vsel_vd_vo_vo_vo_d_d_d_d(o0, o1, o1, d0, d1, d2, d2);
+}
+
+static INLINE vopmask visinf_vo_vd(vdouble d) {
+  return __lasx_xvfcmp_ceq_d(vabs_vd_vd(d), vcast_vd_d(SLEEF_INFINITY));
+}
+
+static INLINE vopmask vispinf_vo_vd(vdouble d) {
+  return __lasx_xvfcmp_ceq_d(d, vcast_vd_d(SLEEF_INFINITY));
+}
+
+static INLINE vopmask visminf_vo_vd(vdouble d) {
+  return __lasx_xvfcmp_ceq_d(d, vcast_vd_d(-SLEEF_INFINITY));
+}
+
+static INLINE vopmask visnan_vo_vd(vdouble d) {
+  return __lasx_xvfcmp_sune_d(d, d);
+}
+
+static INLINE vdouble vgather_vd_p_vi(const double *ptr, vint vi) {
+  int a[VECTLENDP];
+  vstoreu_v_p_vi(a, vi);
+  return (vdouble){ptr[a[0]], ptr[a[1]], ptr[a[2]], ptr[a[3]]};
+}
+
+
+static INLINE vint2 vcast_vi2_vm(vmask vm) { return vm; }
+static INLINE vmask vcast_vm_vi2(vint2 vi) { return vi; }
+
+static INLINE vint2 vrint_vi2_vf(vfloat vf) { return vcast_vi2_vm(__lasx_xvftintrne_w_s(vf)); }
+static INLINE vint2 vtruncate_vi2_vf(vfloat vf) { return vcast_vi2_vm(__lasx_xvftintrz_w_s(vf)); }
+static INLINE vfloat vcast_vf_vi2(vint2 vi) { return __lasx_xvffint_s_w(vcast_vm_vi2(vi)); }
+static INLINE vfloat vcast_vf_f(float f) { return (vfloat){f, f, f, f, f, f, f, f}; }
+static INLINE vint2 vcast_vi2_i(int i) { return __lasx_xvreplgr2vr_w(i); }
+
+static INLINE vmask vreinterpret_vm_vf(vfloat vf)  { return (vmask)vf; }
+static INLINE vfloat vreinterpret_vf_vm(vmask vm)  { return (vfloat)vm; }
+static INLINE vfloat vreinterpret_vf_vi2(vint2 vi) { return (vfloat)vi; }
+static INLINE vint2 vreinterpret_vi2_vf(vfloat vf) { return (vmask)vf; }
+
+static INLINE vdouble vreinterpret_vd_vf(vfloat vf) { return (vdouble)vf; }
+static INLINE vfloat vreinterpret_vf_vd(vdouble vd) { return (vfloat)vd; }
+
+
+static INLINE vfloat vadd_vf_vf_vf(vfloat x, vfloat y) { return __lasx_xvfadd_s(x, y); }
+static INLINE vfloat vsub_vf_vf_vf(vfloat x, vfloat y) { return __lasx_xvfsub_s(x, y); }
+static INLINE vfloat vmul_vf_vf_vf(vfloat x, vfloat y) { return __lasx_xvfmul_s(x, y); }
+static INLINE vfloat vdiv_vf_vf_vf(vfloat x, vfloat y) { return __lasx_xvfdiv_s(x, y); }
+static INLINE vfloat vrec_vf_vf(vfloat x) { return __lasx_xvfrecip_s(x); }
+static INLINE vfloat vsqrt_vf_vf(vfloat x) { return __lasx_xvfsqrt_s(x); }
+static INLINE vfloat vabs_vf_vf(vfloat f) { return  (vfloat)__lasx_xvandn_v((vmask)vcast_vf_f(-0.0f), (vmask)f); }
+static INLINE vfloat vneg_vf_vf(vfloat d) { return  (vfloat)__lasx_xvxor_v((vmask)vcast_vf_f(-0.0f), (vmask)d); }
+static INLINE vfloat vmax_vf_vf_vf(vfloat x, vfloat y) { return __lasx_xvfmax_s(x, y); }
+static INLINE vfloat vmin_vf_vf_vf(vfloat x, vfloat y) { return __lasx_xvfmin_s(x, y); }
+
+static INLINE vfloat vmla_vf_vf_vf_vf(vfloat x, vfloat y, vfloat z) { return __lasx_xvfmadd_s(x, y, z); }
+static INLINE vfloat vmlanp_vf_vf_vf_vf(vfloat x, vfloat y, vfloat z) { return __lasx_xvfnmsub_s(x, y, z); }
+static INLINE vfloat vmlapn_vf_vf_vf_vf(vfloat x, vfloat y, vfloat z) { return __lasx_xvfmsub_s(x, y, z); }
+static INLINE vfloat vfma_vf_vf_vf_vf(vfloat x, vfloat y, vfloat z) { return __lasx_xvfmadd_s(x, y, z); }
+static INLINE vfloat vfmapp_vf_vf_vf_vf(vfloat x, vfloat y, vfloat z) { return __lasx_xvfmadd_s(x, y, z); }
+static INLINE vfloat vfmapn_vf_vf_vf_vf(vfloat x, vfloat y, vfloat z) { return __lasx_xvfmsub_s(x, y, z); }
+static INLINE vfloat vfmanp_vf_vf_vf_vf(vfloat x, vfloat y, vfloat z) { return __lasx_xvfnmsub_s(x, y, z); }
+static INLINE vfloat vfmann_vf_vf_vf_vf(vfloat x, vfloat y, vfloat z) { return __lasx_xvfnmadd_s(x, y, z); }
+
+static INLINE vopmask veq_vo_vf_vf(vfloat x, vfloat y) { return __lasx_xvfcmp_ceq_s(x, y); }
+static INLINE vopmask vneq_vo_vf_vf(vfloat x, vfloat y) { return  __lasx_xvfcmp_cne_s(x, y); }
+static INLINE vopmask vlt_vo_vf_vf(vfloat x, vfloat y) { return  __lasx_xvfcmp_clt_s(x, y); }
+static INLINE vopmask vle_vo_vf_vf(vfloat x, vfloat y) { return __lasx_xvfcmp_cle_s(x, y); }
+static INLINE vopmask vgt_vo_vf_vf(vfloat x, vfloat y) {  return  __lasx_xvfcmp_clt_s(y, x);}
+static INLINE vopmask vge_vo_vf_vf(vfloat x, vfloat y) { return __lasx_xvfcmp_clt_s(y, x);}
+
+static INLINE vint2 vadd_vi2_vi2_vi2(vint2 x, vint2 y) { return __lasx_xvadd_w(x, y); }
+static INLINE vint2 vsub_vi2_vi2_vi2(vint2 x, vint2 y) { return __lasx_xvsub_w(x, y); }
+static INLINE vint2 vneg_vi2_vi2(vint2 e) { return __lasx_xvneg_w(e); }
+static INLINE vint2 vand_vi2_vi2_vi2(vint2 x, vint2 y) { return __lasx_xvand_v(x, y); }
+static INLINE vint2 vandnot_vi2_vi2_vi2(vint2 x, vint2 y) { return __lasx_xvandn_v(x, y); }
+static INLINE vint2 vor_vi2_vi2_vi2(vint2 x, vint2 y) { return __lasx_xvor_v(x, y); }
+static INLINE vint2 vxor_vi2_vi2_vi2(vint2 x, vint2 y) { return __lasx_xvxor_v(x, y); }
+
+static INLINE vint2 vand_vi2_vo_vi2(vopmask x, vint2 y) { return vand_vi2_vi2_vi2(vcast_vi2_vm(x), y); }
+static INLINE vint2 vandnot_vi2_vo_vi2(vopmask x, vint2 y) { return vandnot_vi2_vi2_vi2(vcast_vi2_vm(x), y); }
+
+static INLINE vint2 vsll_vi2_vi2_i(vint2 x, int c) { return __lasx_xvslli_w(x, c); }
+static INLINE vint2 vsrl_vi2_vi2_i(vint2 x, int c) { return __lasx_xvsrli_w(x, c); }
+static INLINE vint2 vsra_vi2_vi2_i(vint2 x, int c) { return __lasx_xvsrai_w(x, c); }
+
+static INLINE vopmask veq_vo_vi2_vi2(vint2 x, vint2 y) { return __lasx_xvseq_w(x, y); }
+static INLINE vopmask vgt_vo_vi2_vi2(vint2 x, vint2 y) { return __lasx_xvslt_w(y, x); }
+static INLINE vint2 veq_vi2_vi2_vi2(vint2 x, vint2 y) { return __lasx_xvseq_w(x, y); }
+static INLINE vint2 vgt_vi2_vi2_vi2(vint2 x, vint2 y) { return __lasx_xvslt_w(y, x); }
+
+static INLINE vint2 vsel_vi2_vo_vi2_vi2(vopmask m, vint2 x, vint2 y) { return __lasx_xvbitsel_v(y, x, m);}
+
+static INLINE vfloat vsel_vf_vo_vf_vf(vopmask o, vfloat x, vfloat y) { return (vfloat)__lasx_xvbitsel_v((vmask)y, (vmask)x, o); }
+
+// At this point, the following three functions are implemented in a generic way,
+// but I will try target-specific optimization later on.
+static INLINE CONST vfloat vsel_vf_vo_f_f(vopmask o, float v1, float v0) {
+  return vsel_vf_vo_vf_vf(o, vcast_vf_f(v1), vcast_vf_f(v0));
+}
+
+static INLINE vfloat vsel_vf_vo_vo_f_f_f(vopmask o0, vopmask o1, float d0, float d1, float d2) {
+  return vsel_vf_vo_vf_vf(o0, vcast_vf_f(d0), vsel_vf_vo_f_f(o1, d1, d2));
+}
+
+static INLINE vfloat vsel_vf_vo_vo_vo_f_f_f_f(vopmask o0, vopmask o1, vopmask o2, float d0, float d1, float d2, float d3) {
+  return vsel_vf_vo_vf_vf(o0, vcast_vf_f(d0), vsel_vf_vo_vf_vf(o1, vcast_vf_f(d1), vsel_vf_vo_f_f(o2, d2, d3)));
+}
+
+static INLINE vopmask visinf_vo_vf(vfloat d) { return veq_vo_vf_vf(vabs_vf_vf(d), vcast_vf_f(SLEEF_INFINITYf)); }
+static INLINE vopmask vispinf_vo_vf(vfloat d) { return veq_vo_vf_vf(d, vcast_vf_f(SLEEF_INFINITYf)); }
+static INLINE vopmask visminf_vo_vf(vfloat d) { return veq_vo_vf_vf(d, vcast_vf_f(-SLEEF_INFINITYf)); }
+static INLINE vopmask visnan_vo_vf(vfloat d) { return __lasx_xvfcmp_sune_s(d,d); }
+
+static INLINE vfloat vgather_vf_p_vi2(const float *ptr, vint2 vi2) {
+  int a[VECTLENSP];
+  vstoreu_v_p_vi2(a, vi2);
+  return (vfloat){ptr[a[0]], ptr[a[1]], ptr[a[2]], ptr[a[3]],
+		       ptr[a[4]], ptr[a[5]], ptr[a[6]], ptr[a[7]]};
+}
+
+#define PNMASK ((vdouble) { +0.0, -0.0, +0.0, -0.0 })
+#define NPMASK ((vdouble) { -0.0, +0.0, -0.0, +0.0 })
+#define PNMASKf ((vfloat) { +0.0f, -0.0f, +0.0f, -0.0f, +0.0f, -0.0f, +0.0f, -0.0f })
+#define NPMASKf ((vfloat) { -0.0f, +0.0f, -0.0f, +0.0f, -0.0f, +0.0f, -0.0f, +0.0f })
+
+static INLINE vdouble vposneg_vd_vd(vdouble d) { return vreinterpret_vd_vm(vxor_vm_vm_vm(vreinterpret_vm_vd(d), vreinterpret_vm_vd(PNMASK))); }
+static INLINE vdouble vnegpos_vd_vd(vdouble d) { return vreinterpret_vd_vm(vxor_vm_vm_vm(vreinterpret_vm_vd(d), vreinterpret_vm_vd(NPMASK))); }
+static INLINE vfloat vposneg_vf_vf(vfloat d) { return vreinterpret_vf_vm(vxor_vm_vm_vm(vreinterpret_vm_vf(d), vreinterpret_vm_vf(PNMASKf))); }
+static INLINE vfloat vnegpos_vf_vf(vfloat d) { return vreinterpret_vf_vm(vxor_vm_vm_vm(vreinterpret_vm_vf(d), vreinterpret_vm_vf(NPMASKf))); }
+
+static INLINE vdouble vsubadd_vd_vd_vd(vdouble x, vdouble y) { return __lasx_xvfadd_d(x, vnegpos_vd_vd(y)); }
+static INLINE vfloat vsubadd_vf_vf_vf(vfloat x, vfloat y) { return __lasx_xvfadd_s(x, vnegpos_vf_vf(y)); }
+
+static INLINE vdouble vmlsubadd_vd_vd_vd_vd(vdouble x, vdouble y, vdouble z) { return vmla_vd_vd_vd_vd(x, y, vnegpos_vd_vd(z)); }
+static INLINE vfloat vmlsubadd_vf_vf_vf_vf(vfloat x, vfloat y, vfloat z) { return vmla_vf_vf_vf_vf(x, y, vnegpos_vf_vf(z)); }
+
+static INLINE vmask vsel_vm_vo64_vm_vm(vopmask o, vmask x, vmask y) { return vor_vm_vm_vm(vand_vm_vm_vm(o, x), vandnot_vm_vm_vm(o, y)); }
+
+static INLINE vmask vsub64_vm_vm_vm(vmask x, vmask y) { return __lasx_xvsub_d(x, y); }
+static INLINE vmask vneg64_vm_vm(vmask x) { return __lasx_xvneg_d(x); }
+static INLINE vopmask vgt64_vo_vm_vm(vmask x, vmask y) {return __lasx_xvslt_d(y,x); } // signed compare
+
+#define vsll64_vm_vm_i(x, c) __lasx_xvslli_d(x, c)
+#define vsrl64_vm_vm_i(x, c) __lasx_xvsrli_d(x, c)
+//@#define vsll64_vm_vm_i(x, c)__lasx_xvslli_d(x, c)
+//@#define vsrl64_vm_vm_i(x, c) __lasx_xvslli_d(x, c)
+
+static INLINE vmask vcast_vm_vi(vint vi) { return vi; } // signed 32-bit => 64-bit
+static INLINE vint vcast_vi_vm(vmask vm) { return vm;  }
+
+static INLINE vmask vreinterpret_vm_vi64(vint64 v) { return v; }
+static INLINE vint64 vreinterpret_vi64_vm(vmask m) { return m; }
+static INLINE vmask vreinterpret_vm_vu64(vuint64 v) { return v; }
+static INLINE vuint64 vreinterpret_vu64_vm(vmask m) { return m; }
diff --git a/src/arch/helperlsx.h b/src/arch/helperlsx.h
new file mode 100644
index 0000000..ad4eae6
--- /dev/null
+++ b/src/arch/helperlsx.h
@@ -0,0 +1,355 @@
+#define ENABLE_DP
+//@#define ENABLE_DP
+#define LOG2VECTLENDP 1
+//@#define LOG2VECTLENDP 1
+#define VECTLENDP (1 << LOG2VECTLENDP)
+//@#define VECTLENDP (1 << LOG2VECTLENDP)
+#define ENABLE_FMA_DP
+//@#define ENABLE_FMA_DP
+
+#define ENABLE_SP
+//@#define ENABLE_SP
+#define LOG2VECTLENSP (LOG2VECTLENDP+1)
+//@#define LOG2VECTLENSP (LOG2VECTLENDP+1)
+#define VECTLENSP (1 << LOG2VECTLENSP)
+//@#define VECTLENSP (1 << LOG2VECTLENSP)
+#define ENABLE_FMA_SP
+//@#define ENABLE_FMA_SP
+
+#define FULL_FP_ROUNDING
+//@#define FULL_FP_ROUNDING
+#define ACCURATE_SQRT
+//@#define ACCURATE_SQRT
+
+#define ISANAME "lsx"
+#define DFTPRIORITY 25
+
+
+#include <lsxintrin.h>
+
+#include <stdint.h>
+#include "misc.h"
+
+
+typedef __m128i vmask;
+typedef __m128i vopmask;
+
+typedef __m128d vdouble;
+typedef __m128i vint;
+
+typedef __m128 vfloat;
+typedef __m128i vint2;
+
+typedef __m128i vint64;
+typedef __m128i vuint64;
+
+typedef struct {
+  vmask x, y;
+} vquad;
+
+typedef vquad vargquad;
+
+
+static INLINE int vavailability_i(int name) { return 3; }
+
+static INLINE int vtestallones_i_vo32(vopmask g) {  return __lsx_bnz_w(g); }
+static INLINE int vtestallones_i_vo64(vopmask g) {  return __lsx_bnz_d(g); }
+
+
+static INLINE vdouble vcast_vd_d(double d) { return (vdouble){d, d}; }
+static INLINE vmask vreinterpret_vm_vd(vdouble vd) { return (vopmask) vd; }
+static INLINE vdouble vreinterpret_vd_vm(vmask vm) { return (vdouble) vm; }
+
+
+static INLINE vint2 vloadu_vi2_p(int32_t *p) { return __lsx_vld((void const *)p, 0);}
+static INLINE void vstoreu_v_p_vi2(int32_t *p, vint2 v) { *((vint2*)p) = v;}
+static INLINE vint vloadu_vi_p(int32_t *p) {return __lsx_vld((void const *)p, 0);}
+static INLINE void vstoreu_v_p_vi(int32_t *p, vint v) { *((vint*)p) = v;}
+static INLINE vfloat vload_vf_p(const float *ptr) {return (vfloat)__lsx_vld((void const *)ptr, 0); }
+static INLINE vfloat vloadu_vf_p(const float *ptr) {return (vfloat)__lsx_vld((void const *)ptr, 0);}
+static INLINE void vstore_v_p_vf(float *ptr, vfloat v) { *((vfloat*)ptr) = v; }
+static INLINE void vstoreu_v_p_vf(float *ptr, vfloat v) { *((vfloat*)ptr) = v; }
+static INLINE vdouble vload_vd_p(const double *ptr) { return (vdouble) __lsx_vld((void const *)ptr, 0);}
+static INLINE vdouble vloadu_vd_p(const double *ptr) { return (vdouble) __lsx_vld((void const *)ptr, 0);}
+static INLINE void vstore_v_p_vd(double *ptr, vdouble v) { *((vdouble*)ptr) = v;}
+static INLINE void vstoreu_v_p_vd(double *ptr, vdouble v) { *((vdouble*)ptr) = v;}
+
+static INLINE vmask vand_vm_vm_vm(vmask x, vmask y) { return __lsx_vand_v(x, y); }
+static INLINE vmask vandnot_vm_vm_vm(vmask x, vmask y) { return __lsx_vandn_v(x, y); }
+static INLINE vmask vor_vm_vm_vm(vmask x, vmask y) { return __lsx_vor_v(x, y); }
+static INLINE vmask vxor_vm_vm_vm(vmask x, vmask y) { return __lsx_vxor_v(x, y); }
+
+static INLINE vopmask vand_vo_vo_vo(vopmask x, vopmask y) { return __lsx_vand_v(x, y); }
+static INLINE vopmask vandnot_vo_vo_vo(vopmask x, vopmask y) { return __lsx_vandn_v(x, y); }
+static INLINE vopmask vor_vo_vo_vo(vopmask x, vopmask y) { return __lsx_vor_v(x, y); }
+static INLINE vopmask vxor_vo_vo_vo(vopmask x, vopmask y) { return __lsx_vxor_v(x, y); }
+
+static INLINE vmask vand_vm_vo64_vm(vopmask x, vmask y) { return __lsx_vand_v(x, y); }
+static INLINE vmask vandnot_vm_vo64_vm(vopmask x, vmask y) { return __lsx_vandn_v(x, y); }
+static INLINE vmask vor_vm_vo64_vm(vopmask x, vmask y) { return __lsx_vor_v(x, y); }
+static INLINE vmask vxor_vm_vo64_vm(vopmask x, vmask y) { return __lsx_vxor_v(x, y); }
+
+static INLINE vmask vand_vm_vo32_vm(vopmask x, vmask y) { return __lsx_vand_v(x, y); }
+static INLINE vmask vandnot_vm_vo32_vm(vopmask x, vmask y) { return __lsx_vandn_v(x, y); }
+static INLINE vmask vor_vm_vo32_vm(vopmask x, vmask y) { return __lsx_vor_v(x, y); }
+static INLINE vmask vxor_vm_vo32_vm(vopmask x, vmask y) { return __lsx_vxor_v(x, y); }
+
+static INLINE vopmask vcast_vo32_vo64(vopmask o) { return __lsx_vshuf4i_w(o,8);}
+
+static INLINE vopmask vcast_vo64_vo32(vopmask o) { return __lsx_vshuf4i_w(o,80);}
+
+static INLINE vopmask vcast_vo_i(int i) {
+  return __lsx_vreplgr2vr_d(i ? -1 : 0);
+}
+
+//towards the nearest even
+static INLINE vdouble vrint_vd_vd(vdouble vd) { return (vdouble)__lsx_vfrintrne_d(vd); }
+static INLINE vint vrint_vi_vd(vdouble vd) { return  __lsx_vftintrne_w_d(vd,vd);}
+static INLINE vfloat vrint_vf_vf(vfloat vd) { return (vfloat)__lsx_vfrintrne_s(vd); }
+
+//towards zero
+static INLINE vint vtruncate_vi_vd(vdouble vd) { return __lsx_vftintrz_w_d(vd,vd);}
+static INLINE vdouble vtruncate_vd_vd(vdouble vd) { return (vdouble)__lsx_vfrintrz_d(vd); }
+static INLINE vfloat vtruncate_vf_vf(vfloat vf) { return (vfloat)__lsx_vfrintrz_s(vf); }
+
+static INLINE vdouble vcast_vd_vi(vint vi) { return  __lsx_vffint_d_l(__lsx_vsllwil_d_w(vi, 0));}
+static INLINE vint vcast_vi_i(int i) { return __lsx_vreplgr2vr_w(i); }
+
+static INLINE vmask vcastu_vm_vi(vint vi) {  return __lsx_vslli_d(__lsx_vsllwil_du_wu(vi, 0), 32);}
+static INLINE vint vcastu_vi_vm(vmask vi) { return __lsx_vshuf4i_w(vi, 221);}
+static INLINE vmask vcast_vm_i_i(int i0, int i1) {
+   return __lsx_vilvl_w(__lsx_vreplgr2vr_w(i0), __lsx_vreplgr2vr_w(i1));
+}
+
+static INLINE vmask vcast_vm_i64(int64_t i) { return __lsx_vreplgr2vr_d(i); }
+static INLINE vmask vcast_vm_u64(uint64_t i) { return __lsx_vreplgr2vr_d((uint64_t)i); }
+
+static INLINE vopmask veq64_vo_vm_vm(vmask x, vmask y) { return __lsx_vseq_d(x, y); }
+static INLINE vmask vadd64_vm_vm_vm(vmask x, vmask y) { return __lsx_vadd_d(x, y); }
+
+
+static INLINE vdouble vadd_vd_vd_vd(vdouble x, vdouble y) { return __lsx_vfadd_d(x, y); }
+static INLINE vdouble vsub_vd_vd_vd(vdouble x, vdouble y) { return __lsx_vfsub_d(x, y); }
+static INLINE vdouble vmul_vd_vd_vd(vdouble x, vdouble y) { return __lsx_vfmul_d(x, y); }
+static INLINE vdouble vdiv_vd_vd_vd(vdouble x, vdouble y) { return __lsx_vfdiv_d(x, y); }
+static INLINE vdouble vrec_vd_vd(vdouble x) { return  __lsx_vfrecip_d(x); }
+static INLINE vdouble vsqrt_vd_vd(vdouble x) { return __lsx_vfsqrt_d(x); }
+static INLINE vdouble vabs_vd_vd(vdouble d) { return (vdouble) __lsx_vandn_v((vopmask)vcast_vd_d(-0.0), (vopmask)d);}
+static INLINE vdouble vneg_vd_vd(vdouble d) { return (vdouble)__lsx_vxor_v((vopmask)vcast_vd_d(-0.0), (vopmask)d); }
+static INLINE vdouble vmla_vd_vd_vd_vd(vdouble x, vdouble y, vdouble z) { return __lsx_vfmadd_d(x, y, z); }
+static INLINE vdouble vmlapn_vd_vd_vd_vd(vdouble x, vdouble y, vdouble z) { return __lsx_vfmsub_d(x, y, z); }
+static INLINE vdouble vmlanp_vd_vd_vd_vd(vdouble x, vdouble y, vdouble z) { return __lsx_vfnmsub_d(x, y, z); }
+static INLINE vdouble vmax_vd_vd_vd(vdouble x, vdouble y) { return __lsx_vfmax_d(x, y); }
+static INLINE vdouble vmin_vd_vd_vd(vdouble x, vdouble y) { return __lsx_vfmin_d(x, y); }
+static INLINE vdouble vlogb_vd_vd(vdouble d)  { return __lsx_vflogb_d(d);}
+
+static INLINE vdouble vfma_vd_vd_vd_vd(vdouble x, vdouble y, vdouble z) { return __lsx_vfmadd_d(x, y, z); }
+static INLINE vdouble vfmapp_vd_vd_vd_vd(vdouble x, vdouble y, vdouble z) { return __lsx_vfmadd_d(x, y, z); }
+static INLINE vdouble vfmapn_vd_vd_vd_vd(vdouble x, vdouble y, vdouble z) { return __lsx_vfmsub_d(x, y, z); }
+static INLINE vdouble vfmanp_vd_vd_vd_vd(vdouble x, vdouble y, vdouble z) { return __lsx_vfnmsub_d(x, y, z); }
+static INLINE vdouble vfmann_vd_vd_vd_vd(vdouble x, vdouble y, vdouble z) { return __lsx_vfnmadd_d(x, y, z); }
+
+static INLINE vopmask veq_vo_vd_vd(vdouble x, vdouble y) { return __lsx_vfcmp_ceq_d(x, y); }
+static INLINE vopmask vneq_vo_vd_vd(vdouble x, vdouble y) { return __lsx_vfcmp_cne_d(x, y); }
+static INLINE vopmask vlt_vo_vd_vd(vdouble x, vdouble y) { return __lsx_vfcmp_clt_d(x, y); }
+static INLINE vopmask vle_vo_vd_vd(vdouble x, vdouble y) { return __lsx_vfcmp_cle_d(x, y); }
+static INLINE vopmask vgt_vo_vd_vd(vdouble x, vdouble y) { return __lsx_vfcmp_clt_d(y, x);}
+static INLINE vopmask vge_vo_vd_vd(vdouble x, vdouble y) { return __lsx_vfcmp_clt_d(y, x); }
+
+
+static INLINE vint vadd_vi_vi_vi(vint x, vint y) { return __lsx_vadd_w(x, y); }
+static INLINE vint vsub_vi_vi_vi(vint x, vint y) { return __lsx_vsub_w(x, y); }
+static INLINE vint vneg_vi_vi(vint e) { return __lsx_vneg_w(e); }
+
+static INLINE vint vand_vi_vi_vi(vint x, vint y) { return __lsx_vand_v(x, y); }
+static INLINE vint vandnot_vi_vi_vi(vint x, vint y) { return __lsx_vandn_v(x, y); }
+static INLINE vint vor_vi_vi_vi(vint x, vint y) { return __lsx_vor_v(x, y); }
+static INLINE vint vxor_vi_vi_vi(vint x, vint y) { return __lsx_vxor_v(x, y); }
+
+static INLINE vint vandnot_vi_vo_vi(vopmask m, vint y) { return __lsx_vandn_v(m, y); }
+static INLINE vint vand_vi_vo_vi(vopmask m, vint y) { return __lsx_vand_v(m, y); }
+
+static INLINE vint vsll_vi_vi_i(vint x, int c) { return __lsx_vslli_w(x, c); }
+static INLINE vint vsrl_vi_vi_i(vint x, int c) { return __lsx_vsrli_w(x, c); }
+static INLINE vint vsra_vi_vi_i(vint x, int c) { return __lsx_vsrai_w(x, c); }
+
+static INLINE vint veq_vi_vi_vi(vint x, vint y) { return __lsx_vseq_w(x, y); }
+static INLINE vint vgt_vi_vi_vi(vint x, vint y) { return __lsx_vslt_w(y,x);}
+
+static INLINE vopmask veq_vo_vi_vi(vint x, vint y) { return   __lsx_vseq_w(x, y); }
+static INLINE vopmask vgt_vo_vi_vi(vint x, vint y) { return __lsx_vslt_w(y, x);}
+
+static INLINE vint vsel_vi_vo_vi_vi(vopmask m, vint x, vint y) { return __lsx_vbitsel_v(y, x, m); }
+
+static INLINE vdouble vsel_vd_vo_vd_vd(vopmask o, vdouble x, vdouble y) { return (vdouble) __lsx_vbitsel_v((vopmask)y, (vopmask)x, o); }
+static INLINE vdouble vsel_vd_vo_d_d(vopmask o, double v1, double v0)
+{
+  return vsel_vd_vo_vd_vd(o, (vdouble){v1, v1}, (vdouble){v0, v0});
+}
+
+static INLINE vdouble vsel_vd_vo_vo_vo_d_d_d_d(vopmask o0, vopmask o1, vopmask o2, double d0, double d1, double d2, double d3)
+{
+   return vsel_vd_vo_vd_vd(o0, vcast_vd_d(d0), vsel_vd_vo_vd_vd(o1, vcast_vd_d(d1), vsel_vd_vo_d_d(o2,d2, d3)));
+}
+static INLINE vdouble vsel_vd_vo_vo_d_d_d(vopmask o0, vopmask o1, double d0, double d1, double d2) {
+  return vsel_vd_vo_vd_vd(o0, vcast_vd_d(d0), vsel_vd_vo_d_d(o1, d1, d2));
+}
+
+static INLINE vopmask visinf_vo_vd(vdouble d) {
+  return __lsx_vfcmp_ceq_d(vabs_vd_vd(d), vcast_vd_d(SLEEF_INFINITY));
+}
+
+static INLINE vopmask vispinf_vo_vd(vdouble d) {
+  return __lsx_vfcmp_ceq_d(d, vcast_vd_d(SLEEF_INFINITY));
+}
+
+static INLINE vopmask visminf_vo_vd(vdouble d) {
+  return __lsx_vfcmp_ceq_d(d, vcast_vd_d(-SLEEF_INFINITY));
+}
+
+static INLINE vopmask visnan_vo_vd(vdouble d) {
+  return __lsx_vfcmp_sune_d(d,d);
+}
+
+static INLINE vdouble vgather_vd_p_vi(const double *ptr, vint vi) {
+  int a[VECTLENDP];
+  vstoreu_v_p_vi(a, vi);
+  return (vdouble){ptr[a[0]], ptr[a[1]]};
+}
+
+static INLINE vint2 vcast_vi2_vm(vmask vm) { return vm; }
+static INLINE vmask vcast_vm_vi2(vint2 vi) { return vi; }
+
+static INLINE vint2 vrint_vi2_vf(vfloat vf) { return vcast_vi2_vm(__lsx_vftintrne_w_s(vf)); }
+static INLINE vint2 vtruncate_vi2_vf(vfloat vf) { return vcast_vi2_vm(__lsx_vftintrz_w_s(vf)); }
+static INLINE vfloat vcast_vf_vi2(vint2 vi) { return __lsx_vffint_s_w(vcast_vm_vi2(vi)); }
+static INLINE vfloat vcast_vf_f(float f) { return (vfloat){f, f, f, f}; }
+static INLINE vint2 vcast_vi2_i(int i) { return __lsx_vreplgr2vr_w(i); }
+
+static INLINE vmask vreinterpret_vm_vf(vfloat vf)  { return (vopmask)vf; }
+static INLINE vfloat vreinterpret_vf_vm(vmask vm)  { return (vfloat)vm; }
+static INLINE vfloat vreinterpret_vf_vi2(vint2 vi) { return (vfloat)vi; }
+static INLINE vint2 vreinterpret_vi2_vf(vfloat vf) { return (vopmask)vf; }
+
+static INLINE vdouble vreinterpret_vd_vf(vfloat vf) { return (vdouble)vf; }
+static INLINE vfloat vreinterpret_vf_vd(vdouble vd) { return (vfloat)vd; }
+
+
+static INLINE vfloat vadd_vf_vf_vf(vfloat x, vfloat y) { return __lsx_vfadd_s(x, y); }
+static INLINE vfloat vsub_vf_vf_vf(vfloat x, vfloat y) { return __lsx_vfsub_s(x, y); }
+static INLINE vfloat vmul_vf_vf_vf(vfloat x, vfloat y) { return __lsx_vfmul_s(x, y); }
+static INLINE vfloat vdiv_vf_vf_vf(vfloat x, vfloat y) { return __lsx_vfdiv_s(x, y); }
+static INLINE vfloat vrec_vf_vf(vfloat x) { return __lsx_vfrecip_s(x); }
+static INLINE vfloat vsqrt_vf_vf(vfloat x) { return __lsx_vfsqrt_s(x); }
+static INLINE vfloat vabs_vf_vf(vfloat f) { return  (vfloat)__lsx_vandn_v((vopmask)vcast_vf_f(-0.0f), (vopmask)f); }
+static INLINE vfloat vneg_vf_vf(vfloat d) { return  (vfloat)__lsx_vxor_v((vopmask)vcast_vf_f(-0.0f), (vopmask)d); }
+static INLINE vfloat vmax_vf_vf_vf(vfloat x, vfloat y) { return __lsx_vfmax_s(x, y); }
+static INLINE vfloat vmin_vf_vf_vf(vfloat x, vfloat y) { return __lsx_vfmin_s(x, y); }
+
+static INLINE vfloat vmla_vf_vf_vf_vf(vfloat x, vfloat y, vfloat z) { return __lsx_vfmadd_s(x, y, z); }
+static INLINE vfloat vmlanp_vf_vf_vf_vf(vfloat x, vfloat y, vfloat z) { return __lsx_vfnmsub_s(x, y, z); }
+static INLINE vfloat vmlapn_vf_vf_vf_vf(vfloat x, vfloat y, vfloat z) { return __lsx_vfmsub_s(x, y, z); }
+static INLINE vfloat vfma_vf_vf_vf_vf(vfloat x, vfloat y, vfloat z) { return __lsx_vfmadd_s(x, y, z); }
+static INLINE vfloat vfmapp_vf_vf_vf_vf(vfloat x, vfloat y, vfloat z) { return __lsx_vfmadd_s(x, y, z); }
+static INLINE vfloat vfmapn_vf_vf_vf_vf(vfloat x, vfloat y, vfloat z) { return __lsx_vfmsub_s(x, y, z); }
+static INLINE vfloat vfmanp_vf_vf_vf_vf(vfloat x, vfloat y, vfloat z) { return __lsx_vfnmsub_s(x, y, z); }
+static INLINE vfloat vfmann_vf_vf_vf_vf(vfloat x, vfloat y, vfloat z) { return __lsx_vfnmadd_s(x, y, z); }
+
+static INLINE vopmask veq_vo_vf_vf(vfloat x, vfloat y) { return __lsx_vfcmp_ceq_s(x, y); }
+static INLINE vopmask vneq_vo_vf_vf(vfloat x, vfloat y) { return  __lsx_vfcmp_cne_s(x, y); }
+static INLINE vopmask vlt_vo_vf_vf(vfloat x, vfloat y) { return  __lsx_vfcmp_clt_s(x, y); }
+static INLINE vopmask vle_vo_vf_vf(vfloat x, vfloat y) { return __lsx_vfcmp_cle_s(x, y); }
+static INLINE vopmask vgt_vo_vf_vf(vfloat x, vfloat y) {  return  __lsx_vfcmp_clt_s(y, x);}
+static INLINE vopmask vge_vo_vf_vf(vfloat x, vfloat y) { return __lsx_vfcmp_clt_s(y, x);}
+
+static INLINE vint2 vadd_vi2_vi2_vi2(vint2 x, vint2 y) { return __lsx_vadd_w(x, y); }
+static INLINE vint2 vsub_vi2_vi2_vi2(vint2 x, vint2 y) { return __lsx_vsub_w(x, y); }
+static INLINE vint2 vneg_vi2_vi2(vint2 e) { return __lsx_vneg_w(e); }
+static INLINE vint2 vand_vi2_vi2_vi2(vint2 x, vint2 y) { return __lsx_vand_v(x, y); }
+static INLINE vint2 vandnot_vi2_vi2_vi2(vint2 x, vint2 y) { return __lsx_vandn_v(x, y); }
+static INLINE vint2 vor_vi2_vi2_vi2(vint2 x, vint2 y) { return __lsx_vor_v(x, y); }
+static INLINE vint2 vxor_vi2_vi2_vi2(vint2 x, vint2 y) { return __lsx_vxor_v(x, y); }
+
+static INLINE vint2 vand_vi2_vo_vi2(vopmask x, vint2 y) { return vand_vi2_vi2_vi2(vcast_vi2_vm(x), y); }
+static INLINE vint2 vandnot_vi2_vo_vi2(vopmask x, vint2 y) { return vandnot_vi2_vi2_vi2(vcast_vi2_vm(x), y); }
+
+static INLINE vint2 vsll_vi2_vi2_i(vint2 x, int c) { return __lsx_vslli_w(x, c); }
+static INLINE vint2 vsrl_vi2_vi2_i(vint2 x, int c) { return __lsx_vsrli_w(x, c); }
+static INLINE vint2 vsra_vi2_vi2_i(vint2 x, int c) { return __lsx_vsrai_w(x, c); }
+
+static INLINE vopmask veq_vo_vi2_vi2(vint2 x, vint2 y) { return __lsx_vseq_w(x, y); }
+static INLINE vopmask vgt_vo_vi2_vi2(vint2 x, vint2 y) { return __lsx_vslt_w(y, x); }
+static INLINE vint2 veq_vi2_vi2_vi2(vint2 x, vint2 y) { return __lsx_vseq_w(x, y); }
+static INLINE vint2 vgt_vi2_vi2_vi2(vint2 x, vint2 y) { return __lsx_vslt_w(y, x); }
+
+static INLINE vint2 vsel_vi2_vo_vi2_vi2(vopmask m, vint2 x, vint2 y) { return __lsx_vbitsel_v(y, x, m);}
+
+static INLINE vfloat vsel_vf_vo_vf_vf(vopmask o, vfloat x, vfloat y) { return (vfloat) __lsx_vbitsel_v((vopmask)y, (vopmask)x, o); }
+
+// At this point, the following three functions are implemented in a generic way,
+// but I will try target-specific optimization later on.
+static INLINE CONST vfloat vsel_vf_vo_f_f(vopmask o, float v1, float v0) {
+  return vsel_vf_vo_vf_vf(o, vcast_vf_f(v1), vcast_vf_f(v0));
+}
+
+static INLINE vfloat vsel_vf_vo_vo_f_f_f(vopmask o0, vopmask o1, float d0, float d1, float d2) {
+  return vsel_vf_vo_vf_vf(o0, vcast_vf_f(d0), vsel_vf_vo_f_f(o1, d1, d2));
+}
+
+static INLINE vfloat vsel_vf_vo_vo_vo_f_f_f_f(vopmask o0, vopmask o1, vopmask o2, float d0, float d1, float d2, float d3) {
+  return vsel_vf_vo_vf_vf(o0, vcast_vf_f(d0), vsel_vf_vo_vf_vf(o1, vcast_vf_f(d1), vsel_vf_vo_f_f(o2, d2, d3)));
+}
+
+static INLINE vopmask visinf_vo_vf(vfloat d) { return veq_vo_vf_vf(vabs_vf_vf(d), vcast_vf_f(SLEEF_INFINITYf)); }
+static INLINE vopmask vispinf_vo_vf(vfloat d) { return veq_vo_vf_vf(d, vcast_vf_f(SLEEF_INFINITYf)); }
+static INLINE vopmask visminf_vo_vf(vfloat d) { return veq_vo_vf_vf(d, vcast_vf_f(-SLEEF_INFINITYf)); }
+static INLINE vopmask visnan_vo_vf(vfloat d) { return __lsx_vfcmp_sune_s(d,d); }
+
+#ifdef _MSC_VER
+// This function is needed when debugging on MSVC.
+static INLINE float vcast_f_vf(vfloat v) {
+
+  return v[0];
+}
+#endif
+
+static INLINE vfloat vgather_vf_p_vi2(const float *ptr, vint2 vi2) {
+  int a[VECTLENSP];
+  vstoreu_v_p_vi2(a, vi2);
+  return (vfloat){ptr[a[0]], ptr[a[1]], ptr[a[2]], ptr[a[3]]};
+}
+
+#define PNMASK ((vdouble) { +0.0, -0.0})
+#define NPMASK ((vdouble) { -0.0, +0.0})
+#define PNMASKf ((vfloat) { +0.0f, -0.0f, +0.0f, -0.0f})
+#define NPMASKf ((vfloat) { -0.0f, +0.0f, -0.0f, +0.0f})
+
+static INLINE vdouble vposneg_vd_vd(vdouble d) { return vreinterpret_vd_vm(vxor_vm_vm_vm(vreinterpret_vm_vd(d), vreinterpret_vm_vd(PNMASK))); }
+static INLINE vdouble vnegpos_vd_vd(vdouble d) { return vreinterpret_vd_vm(vxor_vm_vm_vm(vreinterpret_vm_vd(d), vreinterpret_vm_vd(NPMASK))); }
+static INLINE vfloat vposneg_vf_vf(vfloat d) { return vreinterpret_vf_vm(vxor_vm_vm_vm(vreinterpret_vm_vf(d), vreinterpret_vm_vf(PNMASKf))); }
+static INLINE vfloat vnegpos_vf_vf(vfloat d) { return vreinterpret_vf_vm(vxor_vm_vm_vm(vreinterpret_vm_vf(d), vreinterpret_vm_vf(NPMASKf))); }
+
+static INLINE vdouble vsubadd_vd_vd_vd(vdouble x, vdouble y) { return __lsx_vfadd_d(x, vnegpos_vd_vd(y)); }
+static INLINE vfloat vsubadd_vf_vf_vf(vfloat x, vfloat y) { return __lsx_vfadd_s(x, vnegpos_vf_vf(y)); }
+
+static INLINE vdouble vmlsubadd_vd_vd_vd_vd(vdouble x, vdouble y, vdouble z) { return vmla_vd_vd_vd_vd(x, y, vnegpos_vd_vd(z)); }
+static INLINE vfloat vmlsubadd_vf_vf_vf_vf(vfloat x, vfloat y, vfloat z) { return vmla_vf_vf_vf_vf(x, y, vnegpos_vf_vf(z)); }
+
+static INLINE vmask vsel_vm_vo64_vm_vm(vopmask o, vmask x, vmask y) { return vor_vm_vm_vm(vand_vm_vm_vm(o, x), vandnot_vm_vm_vm(o, y)); }
+
+static INLINE vmask vsub64_vm_vm_vm(vmask x, vmask y) { return __lsx_vsub_d(x, y); }
+static INLINE vmask vneg64_vm_vm(vmask x) { return __lsx_vneg_d(x); }
+static INLINE vopmask vgt64_vo_vm_vm(vmask x, vmask y) {return __lsx_vslt_d(y,x); } // signed compare
+
+#define vsll64_vm_vm_i(x, c) __lsx_vslli_d(x, c)
+#define vsrl64_vm_vm_i(x, c) __lsx_vsrli_d(x, c)
+//@#define vsll64_vm_vm_i(x, c)__lsx_vslli_d(x, c)
+//@#define vsrl64_vm_vm_i(x, c) __lsx_vslli_d(x, c)
+
+static INLINE vmask vcast_vm_vi(vint vi) { return vi; } // signed 32-bit => 64-bit
+static INLINE vint vcast_vi_vm(vmask vm) { return vm;  }
+
+static INLINE vmask vreinterpret_vm_vi64(vint64 v) { return v; }
+static INLINE vint64 vreinterpret_vi64_vm(vmask m) { return m; }
+static INLINE vmask vreinterpret_vm_vu64(vuint64 v) { return v; }
+static INLINE vuint64 vreinterpret_vu64_vm(vmask m) { return m; }
diff --git a/src/arch/helperpurec_scalar.h b/src/arch/helperpurec_scalar.h
index fb83b84..46fc242 100644
--- a/src/arch/helperpurec_scalar.h
+++ b/src/arch/helperpurec_scalar.h
@@ -54,7 +54,7 @@
 #define ENABLE_FMA_SP
 //@#define ENABLE_FMA_SP
 
-#if defined(__AVX2__) || defined(__aarch64__) || defined(__arm__) || defined(__powerpc64__) || defined(__zarch__) || defined(__riscv) || CONFIG == 3
+#if defined(__AVX2__) || defined(__aarch64__) || defined(__arm__) || defined(__powerpc64__) || defined(__zarch__) || defined(__riscv) || defined(__loongarch64) || CONFIG == 3
 #ifndef FP_FAST_FMA
 //@#ifndef FP_FAST_FMA
 #define FP_FAST_FMA
diff --git a/src/libm-tester/CMakeLists.txt b/src/libm-tester/CMakeLists.txt
index 41d6f36..397bb7b 100644
--- a/src/libm-tester/CMakeLists.txt
+++ b/src/libm-tester/CMakeLists.txt
@@ -32,6 +32,9 @@ set(TESTER3_DEFINITIONS_RVVM1NOFMA ATR=cinz_ DPTYPE=vfloat64m1_t SPTYPE=vfloat32
 set(TESTER3_DEFINITIONS_RVVM2      ATR=finz_ DPTYPE=vfloat64m2_t SPTYPE=vfloat32m2_t DPTYPESPEC=dx SPTYPESPEC=fx EXTSPEC=rvvm2 ENABLE_RVVM2)
 set(TESTER3_DEFINITIONS_RVVM2NOFMA ATR=cinz_ DPTYPE=vfloat64m2_t SPTYPE=vfloat32m2_t DPTYPESPEC=dx SPTYPESPEC=fx EXTSPEC=rvvm2nofma ENABLE_RVVM2)
 
+set(TESTER3_DEFINITIONS_LASX          ATR=finz_ DPTYPE=__m256d SPTYPE=__m256 DPTYPESPEC=d4 SPTYPESPEC=f8  EXTSPEC=lasx)
+set(TESTER3_DEFINITIONS_LSX           ATR=finz_ DPTYPE=__m128d SPTYPE=__m128 DPTYPESPEC=d2 SPTYPESPEC=f4  EXTSPEC=lsx)
+
 set(TESTER3_DEFINITIONS_PUREC_SCALAR    ATR=cinz_ DPTYPE=double SPTYPE=float DPTYPESPEC=d1 SPTYPESPEC=f1 EXTSPEC=purec)
 set(TESTER3_DEFINITIONS_PURECFMA_SCALAR ATR=finz_ DPTYPE=double SPTYPE=float DPTYPESPEC=d1 SPTYPESPEC=f1 EXTSPEC=purecfma)
 
@@ -55,6 +58,9 @@ elseif(CMAKE_SYSTEM_PROCESSOR MATCHES "s390x")
 elseif(CMAKE_SYSTEM_PROCESSOR MATCHES "riscv64")
   set(TEST3_CINZ purec_scalar rvvm1nofma rvvm2nofma)
   set(TEST3_FINZ purecfma_scalar rvvm1 rvvm2)
+elseif(CMAKE_SYSTEM_PROCESSOR MATCHES "loongarch64")
+  set(TEST3_CINZ purec_scalar  )
+  set(TEST3_FINZ purecfma_scalar lsx lasx)
 endif()
 
 #
diff --git a/src/libm-tester/gnuabi_compatibility.c b/src/libm-tester/gnuabi_compatibility.c
index 3e168c5..45d86b9 100644
--- a/src/libm-tester/gnuabi_compatibility.c
+++ b/src/libm-tester/gnuabi_compatibility.c
@@ -113,6 +113,36 @@ typedef svint32_t vint;
 typedef svint32_t vint2;
 #endif /* ENABLE_SVE */
 
+#ifdef ENABLE_LSX
+#include <lsxintrin.h>
+
+#define ISA_TOKEN b
+#define VLEN_SP 4
+#define VLEN_DP 2
+#define VECTOR_CC
+
+typedef __m128i vopmask;
+typedef __m128d vdouble;
+typedef __m128  vfloat;
+typedef __m128i vint;
+typedef __m128i vint2;
+#endif /* ENABLE_LSX */
+
+#ifdef ENABLE_LASX
+#include <lasxintrin.h>
+
+#define ISA_TOKEN d
+#define VLEN_SP 8
+#define VLEN_DP 4
+#define VECTOR_CC
+
+typedef __m256i vopmask;
+typedef __m256d vdouble;
+typedef __m256 vfloat;
+typedef __m256i vint;
+typedef __m256i vint2;
+#endif /* ENABLE_LASX */
+
 // GNUABI name mangling macro.
 #ifndef MASKED_GNUABI
 
diff --git a/src/libm-tester/iutsimd.c b/src/libm-tester/iutsimd.c
index 03fcd74..66f56d3 100644
--- a/src/libm-tester/iutsimd.c
+++ b/src/libm-tester/iutsimd.c
@@ -31,7 +31,7 @@
 #include <float.h>
 #include <limits.h>
 
-#if defined(__AVX2__) || defined(__aarch64__) || defined(__arm__) || defined(__powerpc64__)
+#if defined(__AVX2__) || defined(__aarch64__) || defined(__arm__) || defined(__powerpc64__) || defined(__loongarch64)
 #ifndef FP_FAST_FMA
 #define FP_FAST_FMA
 #endif
@@ -65,6 +65,14 @@
 #include <vecintrin.h>
 #endif
 
+#if defined(__loongarch_asx)
+#include <lasxintrin.h>
+#endif
+
+#if defined(__loongarch_sx)
+#include <lsxintrin.h>
+#endif
+
 #define SLEEF_ALWAYS_INLINE inline
 #define SLEEF_INLINE
 #define SLEEF_CONST
@@ -367,6 +375,26 @@ typedef Sleef_SLEEF_VECTOR_FLOAT_2 vfloat2;
 #include "renamervvm2nofma.h"
 #endif
 
+#ifdef ENABLE_LASX
+#include "renamelasx.h"
+#if !defined(USE_INLINE_HEADER)
+#define CONFIG 1
+#include "helperlasx.h"
+typedef Sleef___m256d_2 vdouble2;
+typedef Sleef___m256_2 vfloat2;
+#endif
+#endif
+
+#ifdef ENABLE_LSX
+#include "renamelsx.h"
+#if !defined(USE_INLINE_HEADER)
+#define CONFIG 1
+#include "helperlsx.h"
+typedef Sleef___m128d_2 vdouble2;
+typedef Sleef___m128_2 vfloat2;
+#endif
+#endif
+
 #ifdef ENABLE_PUREC_SCALAR
 #include "renamepurec_scalar.h"
 #if !defined(USE_INLINE_HEADER)
diff --git a/src/libm-tester/tester2simddp.c b/src/libm-tester/tester2simddp.c
index 5071bb7..a924538 100644
--- a/src/libm-tester/tester2simddp.c
+++ b/src/libm-tester/tester2simddp.c
@@ -223,6 +223,22 @@ typedef Sleef_SLEEF_VECTOR_FLOAT_2 vfloat2;
 #include "sleef.h"
 #endif
 
+#ifdef ENABLE_LASX
+#define CONFIG 1
+#include "helperlasx.h"
+#include "renamelasx.h"
+typedef Sleef___m256d_2 vdouble2;
+typedef Sleef___m256_2 vfloat2;
+#endif
+
+#ifdef ENABLE_LSX
+#define CONFIG 1
+#include "helperlsx.h"
+#include "renamelsx.h"
+typedef Sleef___m128d_2 vdouble2;
+typedef Sleef___m128_2 vfloat2;
+#endif
+
 #ifdef ENABLE_PUREC_SCALAR
 #define CONFIG 1
 #include "helperpurec_scalar.h"
diff --git a/src/libm-tester/tester2simdsp.c b/src/libm-tester/tester2simdsp.c
index 3fb1e61..064908e 100644
--- a/src/libm-tester/tester2simdsp.c
+++ b/src/libm-tester/tester2simdsp.c
@@ -223,6 +223,23 @@ typedef Sleef_SLEEF_VECTOR_FLOAT_2 vfloat2;
 #include "sleef.h"
 #endif
 
+
+#ifdef ENABLE_LASX
+#define CONFIG 1
+#include "helperlasx.h"
+#include "renamelasx.h"
+typedef Sleef___m256d_2 vdouble2;
+typedef Sleef___m256_2 vfloat2;
+#endif
+
+#ifdef ENABLE_LSX
+#define CONFIG 1
+#include "helperlsx.h"
+#include "renamelsx.h"
+typedef Sleef___m128d_2 vdouble2;
+typedef Sleef___m128_2 vfloat2;
+#endif
+
 #ifdef ENABLE_PUREC_SCALAR
 #define CONFIG 1
 #include "helperpurec_scalar.h"
diff --git a/src/libm-tester/tester3.c b/src/libm-tester/tester3.c
index a55404e..0615943 100644
--- a/src/libm-tester/tester3.c
+++ b/src/libm-tester/tester3.c
@@ -139,6 +139,20 @@ static vfloat64m2_t vd2gety_vd_vd2(vfloat64m4_t v) { return __riscv_vget_f64m2(v
 #undef VECTLENDP
 #endif
 
+#if defined(__loongarch_sx)
+static INLINE __m128d set__m128d(double d, int r) { static double a[2]; memrand(a, sizeof(a)); a[r & 1] = d; return (__m128d)__lsx_vld((void const *)a, 0); }
+static INLINE double get__m128d(__m128d v, int r) { static double a[2]; __lsx_vst(v, (void *)a, 0); return unifyValue(a[r & 1]); }
+static INLINE __m128 set__m128(float d, int r) { static float a[4]; memrand(a, sizeof(a)); a[r & 3] = d; return (__m128)__lsx_vld((void const *)a, 0); }
+static INLINE float get__m128(__m128 v, int r) { static float a[4]; __lsx_vst(v, (void *)a, 0); return unifyValuef(a[r & 3]); }
+#endif
+
+#if defined(__loongarch_asx)
+static INLINE __m256d set__m256d(double d, int r) { static double a[4]; memrand(a, sizeof(a)); a[r & 3] = d; return (__m256d)__lasx_xvld((void const *)a, 0); }
+static INLINE double get__m256d(__m256d v, int r) { static double a[4]; __lasx_xvst(v, (void *)a, 0); return unifyValue(a[r & 3]); }
+static INLINE __m256 set__m256(float d, int r) { static float a[8]; memrand(a, sizeof(a)); a[r & 7] = d; return (__m256)__lasx_xvld((void const *)a, 0); }
+static INLINE float get__m256(__m256 v, int r) { static float a[8]; __lasx_xvst(v, (void *)a, 0);; return unifyValuef(a[r & 7]); }
+#endif
+
 //
 
 // ATR = cinz_, NAME = sin, TYPE = d2, ULP = u35, EXT = sse2
diff --git a/src/libm/CMakeLists.txt b/src/libm/CMakeLists.txt
index a0e4029..479ffa0 100644
--- a/src/libm/CMakeLists.txt
+++ b/src/libm/CMakeLists.txt
@@ -69,6 +69,16 @@ elseif(SLEEF_ARCH_RISCV64)
     PUREC_SCALAR
     PURECFMA_SCALAR
     )
+elseif(SLEEF_ARCH_LOONGARCH64)
+  set(SLEEF_HEADER_LIST
+    LASX_
+    LASX
+    LSX
+    LSX_
+    PUREC_SCALAR
+    PURECFMA_SCALAR
+    DSP_SCALAR
+  )
 endif()
 
 # HEADER_PARAMS
@@ -112,6 +122,11 @@ command_arguments(HEADER_PARAMS_RVVM1NOFMA      cinz_ x x vfloat64m1_t vfloat32m
 command_arguments(HEADER_PARAMS_RVVM2           finz_ x x vfloat64m2_t vfloat32m2_t vint32m1_t vint32m2_t __riscv_v rvvm2)
 command_arguments(HEADER_PARAMS_RVVM2NOFMA      cinz_ x x vfloat64m2_t vfloat32m2_t vint32m1_t vint32m2_t __riscv_v rvvm2nofma)
 
+command_arguments(HEADER_PARAMS_LASX_           _ 4 8 __m256d __m256 __m256i __m256i  __loongarch_asx )
+command_arguments(HEADER_PARAMS_LASX            finz_ 4 8 __m256d __m256 __m256i __m256i  __loongarch_asx lasx)
+command_arguments(HEADER_PARAMS_LSX             finz_ 2 4 __m128d __m128 __m128i __m128i  __loongarch_sx lsx)
+command_arguments(HEADER_PARAMS_LSX_            _ 2 4 __m128d __m128 __m128i __m128i  __loongarch_sx )
+
 command_arguments(HEADER_PARAMS_DSP_SCALAR      -     1 1 double float int32_t int32_t __STDC__)
 command_arguments(HEADER_PARAMS_PUREC_SCALAR    cinz_ 1 1 double float int32_t int32_t __STDC__ purec)
 command_arguments(HEADER_PARAMS_PURECFMA_SCALAR finz_ 1 1 double float int32_t int32_t __STDC__ purecfma)
@@ -138,6 +153,8 @@ command_arguments(RENAME_PARAMS_VXE             finz_ 2 4 vxe)
 command_arguments(RENAME_PARAMS_VXENOFMA        cinz_ 2 4 vxenofma)
 command_arguments(RENAME_PARAMS_VXE2            finz_ 2 4 vxe2)
 command_arguments(RENAME_PARAMS_VXE2NOFMA       cinz_ 2 4 vxe2nofma)
+command_arguments(RENAME_PARAMS_LASX            finz_ 4 8 lasx)
+command_arguments(RENAME_PARAMS_LSX             finz_ 2 4 lsx)
 command_arguments(RENAME_PARAMS_PUREC_SCALAR    cinz_ 1 1 purec)
 command_arguments(RENAME_PARAMS_PURECFMA_SCALAR finz_ 1 1 purecfma)
 command_arguments(RENAME_PARAMS_CUDA            finz_ 1 1 cuda)
@@ -163,6 +180,9 @@ command_arguments(RENAME_PARAMS_RVVM1NOFMA      cinz_ x x rvvm1nofma)
 command_arguments(RENAME_PARAMS_RVVM2           finz_ x x rvvm2)
 command_arguments(RENAME_PARAMS_RVVM2NOFMA      cinz_ x x rvvm2nofma)
 
+command_arguments(RENAME_PARAMS_GNUABI_LASX     lasx d 4 8 __m256d __m256 __m256i __m256i  __loongarch_asx)
+command_arguments(RENAME_PARAMS_GNUABI_LSX      lsx b 2 4 __m128d __m128 __m128i __m128i  __loongarch_sx)
+
 # ALIAS_PARAMS
 
 command_arguments(ALIAS_PARAMS_AVX512F_DP   8 __m512d __m256i e avx512f)
diff --git a/src/libm/sleeflibm_header.h.org.in b/src/libm/sleeflibm_header.h.org.in
index 89b3a1c..3537b2c 100644
--- a/src/libm/sleeflibm_header.h.org.in
+++ b/src/libm/sleeflibm_header.h.org.in
@@ -21,7 +21,7 @@
 #define SLEEF_INLINE __forceinline
 #endif
 
-#if defined(__AVX2__) || defined(__aarch64__) || defined(__arm__) || defined(__powerpc64__) || defined(__zarch__)
+#if defined(__AVX2__) || defined(__aarch64__) || defined(__arm__) || defined(__powerpc64__) || defined(__zarch__) ||  defined(__loongarch64)
 #ifndef FP_FAST_FMA
 #define FP_FAST_FMA
 #endif
@@ -86,6 +86,14 @@ typedef __vector long long SLEEF_VECTOR_LONGLONG;
 typedef __vector unsigned long long SLEEF_VECTOR_ULONGLONG;
 #endif
 
+#if defined(__loongarch_sx)
+#include <lsxintrin.h>
+#endif
+
+#if defined(__loongarch_asx)
+#include <lasxintrin.h>
+#endif
+
 //
 
 #if defined(SLEEF_ENABLE_OMP_SIMD) && (defined(__GNUC__) || defined(__CLANG__)) && !defined(__INTEL_COMPILER)
@@ -101,6 +109,12 @@ typedef __vector unsigned long long SLEEF_VECTOR_ULONGLONG;
 #elif defined(__x86_64__) && defined(__SSE2__)
 #define SLEEF_PRAGMA_OMP_SIMD_DP _Pragma ("omp declare simd simdlen(2) notinbranch")
 #define SLEEF_PRAGMA_OMP_SIMD_SP _Pragma ("omp declare simd simdlen(4) notinbranch")
+#elif defined(__loongarch64) && defined(__loongarch_asx)
+#define SLEEF_PRAGMA_OMP_SIMD_DP _Pragma ("omp declare simd simdlen(4) notinbranch")
+#define SLEEF_PRAGMA_OMP_SIMD_SP _Pragma ("omp declare simd simdlen(8) notinbranch")
+#elif defined(__loongarch64) && defined(__loongarch_sx)
+#define SLEEF_PRAGMA_OMP_SIMD_DP _Pragma ("omp declare simd simdlen(2) notinbranch")
+#define SLEEF_PRAGMA_OMP_SIMD_SP _Pragma ("omp declare simd simdlen(4) notinbranch")
 #endif
 #endif
 
diff --git a/src/libm/sleefsimddp.c b/src/libm/sleefsimddp.c
index 6cdaa6a..7fe5e3b 100644
--- a/src/libm/sleefsimddp.c
+++ b/src/libm/sleefsimddp.c
@@ -274,6 +274,32 @@ extern const double Sleef_rempitabdp[];
 #endif
 #endif /* ENABLE_RVVM2NOFMA */
 
+// LoongArch
+
+#ifdef ENABLE_LASX
+#define CONFIG 1
+#include "helperlasx.h"
+#ifdef DORENAME
+#ifdef ENABLE_GNUABI
+#include "renamelasx_gnuabi.h"
+#else
+#include "renamelasx.h"
+#endif
+#endif
+#endif
+
+#ifdef ENABLE_LSX
+#define CONFIG 1
+#include "helperlsx.h"
+#ifdef DORENAME
+#ifdef ENABLE_GNUABI
+#include "renamelsx_gnuabi.h"
+#else
+#include "renamelsx.h"
+#endif
+#endif
+#endif
+
 // Generic
 
 #ifdef ENABLE_VECEXT
diff --git a/src/libm/sleefsimdsp.c b/src/libm/sleefsimdsp.c
index c5dbc2a..48efedc 100644
--- a/src/libm/sleefsimdsp.c
+++ b/src/libm/sleefsimdsp.c
@@ -374,6 +374,40 @@ extern const float Sleef_rempitabsp[];
 #endif
 #endif
 
+// LoongArch
+
+#ifdef ENABLE_LASX
+#define CONFIG 1
+#if !defined(SLEEF_GENHEADER)
+#include "helperlasx.h"
+#else
+#include "macroonlylasx.h"
+#endif
+#ifdef DORENAME
+#ifdef ENABLE_GNUABI
+#include "renamelasx_gnuabi.h"
+#else
+#include "renamelasx.h"
+#endif
+#endif
+#endif
+
+#ifdef ENABLE_LSX
+#define CONFIG 1
+#if !defined(SLEEF_GENHEADER)
+#include "helperlsx.h"
+#else
+#include "macroonlylsx.h"
+#endif
+#ifdef DORENAME
+#ifdef ENABLE_GNUABI
+#include "renamelsx_gnuabi.h"
+#else
+#include "renamelsx.h"
+#endif
+#endif
+#endif
+
 // Generic
 
 #ifdef ENABLE_VECEXT
-- 
2.20.1

